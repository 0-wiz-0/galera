Codership Oy
http://www.codership.com
<info@codership.com>

DISCLAIMER

THIS SOFTWARE PROVIDED "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER
EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.
IN NO EVENT SHALL CODERSHIP OY BE HELD LIABLE TO ANY PARTY FOR ANY DAMAGES
RESULTING DIRECTLY OR INDIRECTLY FROM THE USE OF THIS SOFTWARE.

Trademark Information.

MySQL is a trademark or registered trademark of Sun Microsystems, Inc. or
its subsidiaries in the US and other countries. Other marks are the property
of their respective owners.

Licensing Information.

Please see ./mysql/LICENSE.mysql and ./galera/LICENSE.galera

The WSREP patch for MySQL can be obtained at: bzr branch lp:codership-mysql


ABOUT THIS DOCUMENT

This document only covers issues specific to MySQL/Galera distribution. It does
not cover the use or administration of MySQL server per se. The reader is
assumed to know how to install, configure, administer and use MySQL server.


CONTENTS:
=========
0. WHAT IS MYSQL/GALERA CLUSTER
1. CLUSTER SETUP
   1.1 INSTALLATION
   1.2 STARTING GROUP COMMUNICATION DAEMON
   1.3 STARTING MYSQL SERVERS
2. USING THE CLUSTER
   2.1 LOADING DATA TO A CLUSTER
   2.2 CONNECTING APPLICATION TO A CLUSTER
   2.3 LOAD BALANCER
   2.4 ADDING NEW NODE TO A CLUSTER
3. CONFIGURATION
   3.1 MANDATORY OPTIONS
   3.2 WSREP OPTIONS
4. Using MySQL/Galera in Amazon EC2
5. LIMITATIONS


0. WHAT IS MYSQL/GALERA CLUSTER v0.7pre

MySQL/Galera cluster is a synchronous multi-master MySQL server cluster.
In this release it consists of a single group communication daemon vsbes and several MySQL server instances which connect to it in a star-like fashion:

     ,--------.   ,---------.   ,--------.
     | mysqld |---|  vsbes  |---| mysqld |
     `--------'   `---------'   `--------'
                   /       \
            ,--------.   ,--------.
            | mysqld |   | mysqld |
            `--------'   `--------'

Server states are synchronized by replicating transaction changes at commit
time. The cluster is virtually synchronous: this means that each node commits
transactions in exactly the same order, although not necessarily at the same
physical moment. (The latter is not that important as it may seem, since in most
cases DBMS gives no guarantee on when the transaction is actually executed.)
Built-in flow control keeps nodes within fraction of a second from each other,
this is more than enough for most practical purposes.

Main features of MySQL/Galera cluster:

* Truly highly available: no transaction is ever lost in case of a node crash. 
  All nodes always have consistent state.

* True multi-master: all cluster nodes can handle WRITE load concurrently.

* Highly transparent: the cluster is intended as a drop-in replacement for a 
  single MySQL server. (See LIMITATIONS below)

* Scalable even with WRITE-intensive applications.

This distribution package contains all software you'll need to setup
MySQL/Galera cluster. It is essentially a self-contained MySQL server
installation with its own configuration file, data directory and preconfigured
empty database. You don't need administrative privileges or to uninstall/
disable previously installed MySQL software to use this distribution.


1. CLUSTER SETUP
               
To setup MySQL/Galera cluster you will need several networked computers -
one for each mysqld instance you plan to use. For best performance those
computers should be of approximately same configuration: Galera replication is
synchronous and one slow machine will slow down the whole cluster.

You must also select one machine that would host group communication daemon
(hereafter arbitrator). For best performance it should be a separate computer,
but it also can be one of the MySQL nodes. The only requirement is that
arbitrator is accessible to all other cluster nodes.

It takes 3 steps to set up the cluster:

1) Copy this distribution to all prospective nodes of the cluster and unpack it
   to location of your choice.

2) Start group communication daemon on arbitrator node by running vsbes script
   from the distribution.

3) Start MySQL servers.

(NOTE: You can easily set up the cluster on a single computer. However this
makes little sense, as you won't see the the benefits of high availability and
scalability. Hence it is not covered by this document.)


1.1 INSTALLATION

Just copy and unpack the distribution on the prospective cluster nodes to
wherever you have privileges. The distribution was designed to be able to run
on most systems without reconfiguration. It is a self-contained MySQL 
installation and comes with its own data directory and a preconfigured empty 
database with users 'root' (password 'rootpass') and 'test' (password 
'testpass', privileges on schema 'test'). As a result default installation will 
require at least 1Gb of free space for InnoDB files (will be created on first 
start).

This requirement, as well as other MySQL and Galera options can be changed by
editing configuration file which can be found at <INST_ROOT>/mysql/etc/my.cnf.

Please see CONFIGURATION chapter for the details on editable parameters.


1.2 STARTING GROUP COMMUNICATION DAEMON

Select one of the nodes to act as an arbitrator. On that node issue the
following command:

   <INST_ROOT>/vsbes start

This will start a Galera group communication daemon listening on all interfaces
at port 4567. GCS_ADDRESS environment variable can be used to bind the daemon
to particular interface or port, e.g.:

   GCS_ADDRESS=192.168.0.1:3333 <INST_ROOT>/vsbes start


1.3 STARTING MYSQL SERVERS

<INST_ROOT>/mysql-galera is a special MySQL startup script that sets proper
options (including data directory path) for mysqld. If you're running it as a
superuser, you have to make sure there is 'mysql' user in the system and it has
sufficient privileges on the installation directory (see MySQL Manual about
running mysqld as root). 

You will need to tell mysqld the address of the group communication daemon. For
this distribution the address follows this layout:

   vsbes://<IP address>:<port>.

Thus, if you have started vsbes on the node with IP address 192.168.1.1, then
GCS address would be vsbes://192.168.1.1:4567

1) Either supply the address as the option on the command line:

   <INST_ROOT>/mysql-galera -g gcomm://192.168.1.1:4567 start

2) Or you can hardcode it in <INST_ROOT>/my.cnf by setting
   wsrep_gcs_address="gcomm://192.168.1.1:4567" and then just launch
   mysql-galera script:

   <INST_ROOT>/mysql-galera start

It might take few minutes to start mysqld for the first time as it will have to
create required InnoDB files.

For full description of mysql-galera options and commands see:

   <INST_ROOT>/mysql-galera --help


2. USING THE CLUSTER

After you have successfully started all cluster nodes, the cluster is ready to
use.

From the client point of view each cluster node works like a usual MySQL server
- client-side application does not have to be changed in any way. Each node can
be accessed independently and asynchronously. Just direct SQL load to any one or
more of the cluster nodes. For most practical purposes you can treat
MySQL/Galera cluster as a single MySQL server listening on multiple interfaces
with the exception that you might see transaction deadlocks where you previously
didn't.

2.1 LOADING DATA TO CLUSTER

Initially distribution database is empty. You can populate it by loading the
dump of your data to any one of the nodes. It will be automatically replicated
to others. Please note that this release supports only InnoDB storage engine.

2.2 CONNECTING APPLICATION TO CLUSTER

As was mentioned above, for the client application each node looks like a normal
MySQL server and can be used independently. This creates considerable
flexibility in the way the cluster can be utilized. The approaches can be
categorized in three groups:

1) Seeking High Availability only. In this case client application connects to
   only one node, the rest serving as hot backups:

   ,-------------.
   | application |
   `-------------'
        | | |        DB backups
      ,-------. ,-------. ,-------.
      | node1 | | node2 | | node3 |
      `-------' `-------' `-------'
       <===== cluster nodes =====>

   In the case of primary node failure application can instantly switch to
   another node.

2) Seeking High Availability and improved performance through uniform load
   distribution. If there are several client connections to the database, they
   can be uniformly distributed between cluster nodes resulting in better
   performance. The exact degree of performance improvement depends on
   application's SQL profile. Note, that transaction rollback rate may also 
   increase.

             ,-------------.
             | application |
             `-------------'
             /      |      \
      ,-------. ,-------. ,-------.
      | node1 | | node2 | | node3 |
      `-------' `-------' `-------'
       <===== cluster nodes =====>

   In the case of a node failure application can keep on using the remaining 
   healthy nodes.

   In this setup application can also be clustered with a dedicated application
   instance per database node, thus achieving HA not only for the database,
   but for the whole application stack:

             ,-------------.
             |   clients   |
             `-------------'
             /      |      \
      ,------.   ,------.   ,------.
      | app1 |   | app2 |   | app3 |
      `------'   `------'   `------'
         |          |          |
     ,-------.  ,-------.  ,-------.
     | node1 |  | node2 |  | node3 |
     `-------'  `-------'  `-------'
      <====== cluster nodes ======>

3) Seeking High Availability and improved performance through smart load
   distribution. Uniform load distribution can cause undesirably high rollback
   rate. Directing transactions which access the same set of tables to the
   same node can considerably improve performance by reducing the number of
   rollbacks. Also, if your application can distinguish between read/write and
   read-only transactions, the following configuration may be quite efficient:

             ,---------------------.
             |     application     |
             `---------------------'
       writes /         | reads    \ reads
      ,-------.     ,-------.     ,-------.
      | node1 |     | node2 |     | node3 |
      `-------'     `-------'     `-------'
       <======== cluster nodes ==========>


2.3 LOAD BALANCER

If your application cannot utilize several database servers (most don't) you
will need to use SQL proxy or a TCP load balancer to distribute load between
the MySQL/Galera nodes. This is needed not only to increase performance, but
also for a quick switch in case of a node failure. If performance of your
application is DBMS-bound, you can run the balancer on the same machine as
application/client. Be aware, however, that SQL load balancing might be a CPU
hungry operation: usually SQL traffic consists of many small packets. For best
results we recommend to carefully examine CPU consumption of the balancer and
if needed dedicate a separate machine for it.

Unlike traditional MySQL master-slave cluster MySQL/Galera cluster does not
require any SQL traffic filtering, it is highly transparent and plain TCP
connection balancer would suffice.

TCP connection balancers that were successfully used with MySQL/Galera:
- Pen (http://siag.nu/pen/)
- GLB (http://www.codership.com/en/downloads/glb) 


2.4 ADDING NEW NODE TO A CLUSTER




3. CONFIGURATION

Each MySQL/Galera node is configured just like the usual MySQL server, we just
added few configuration variables to my.cnf. In addition some options can be
passed to mysql-galera startup script (see mysql-galera --help).

3.1 MANDATORY CONFIGURATION OPTIONS

binlog_format=ROW
This option is required to use row-level replication as opposed to
statement-level. For performance and consistency considerations don't change
that. As a side effect, binlog, if tuned on, can be ROW only. In future this
option won't have special meaning.

Mandatory options are hardcoded both in distribution's my.cnf file and in
mysql-galera script.

3.2 WSREP OPTIONS

Here WSREP stands for Write-Set REPlication - a synchronous replication API
that Codership is developing for transactional databases. Galera is a library
that implements WSREP services. Default values are shown.

wsrep_provider=none
   A full path to the library that implements WSREP interface. If none is
   specified, the server behaves almost like a normal mysqld, with slight
   overhead. mysql-galera script automatically substitutes it to point to
   Galera implementation shipped with the demo. It can be overridden with
   WSREP environment variable.

wsrep_gcs_address="dummy://"
   Group Communication System address. Depends on the WSREP provider. Galera
   recognises "dummy://" and "gcomm://<IP address>:<IP port>"

wsrep_gcs_group="my_cluster"
   Logical group name, must be the same for all nodes of the cluster.

wsrep_slave_threads=1
   Number of threads dedicated to processing of writesets from other nodes.
   Values greater than 1 not tested.

wsrep_dbug_option
   Options for the built-in DBUG library (independent from what MySQL uses).
   Empty by default.

wsrep_local_cache_size=20M
   Amount of RAM reserved for writeset cache in bytes. Excess writesets are
   stored on the hard drive in MySQL data directory

wsrep_ws_persistency=0
   Save writesets in binary files. For debugging purposes only.

wsrep_debug=0
   Enable debug-level logging.  

NEW since demo1:

wsrep_convert_LOCK_to_trx=1
   Implicitly convert locking sessions into transactions inside mysqld. By
   itself it does not mean support for locking sessions, but it prevents the
   database from going into logically inconsistent state.
   
wsrep_retry_autocommit=1
   Retry autocommit queries and single statement transactions should they fail
   certification test. This is analogous to rescheduling an autocommit query
   should it go into deadlock with other transactions in the database lock
   manager.
   
wsrep_auto_increment_control=1
   Automatically adjust auto_increment_increment and auto_increment_offset
   variables based on the number of nodes in the cluster. Significantly reduces
   certification conflic rate for INSERTS.

New since demo2:

innodb_autoinc_lock_mode=2
   This is a required parameter. Without it INSERTs into tables with
   AUTO_INCREMENT column may fail.

wsrep_drupal_282555_workaround=1
   MySQL seems to have an obscure bug when INSERT into table with
   AUTO_INCREMENT column with NULL value for that column may fail due to
   duplicate key. When this option is on, it retries such INSERTs. Required for
   Drupal. Documented at:
      http://bugs.mysql.com/bug.php?id=41984
      http://drupal.org/node/282555

4. Using MySQL/Galera demo in Amazon EC2

MySQL/Galera demo works anywhere TCP/IP works. Therefore using MySQL/Galera
demo in Amazon EC2 environment is no different than in LAN. Just launch several
instances of your favourite AMI, copy and unpack the distribution, start gcomm
on one of them and then start servers telling them where to find the
arbitrator. Don't forget to use external arbitrator address if your nodes are 
running in different accessibility zones (obviously running in different 
accessibility zones degrades performance somewhat).

NOTE: this demo may be binary incompatible with some older Linux distributions.
      Please use CentOS 5.0 or newer.

5. LIMITATIONS

1) Currently replication works only with InnoDB storage engine. Any writes to 
   tables of other types, including system (mysql.*) tables are not replicated. 
   E.g. if you want to add another user, you have to do it manually on every
   node where you want that user to connect to.

2) Rows in tables without primary keys may appear in different order on
   different nodes. As a result SELECT...LIMIT... may return slightly different
   sets.

3) Currently MySQL/Galera demo lacks automatic state transfer. Every time 
   you add a new node to the cluster and/or restart a node you need to manually 
   synchronize database contents on the nodes. See README for details.

4) Not replicated in this demo:
   CREATE/DROP anything but TABLE/DATATBASE.
   GRANT privileges.
   Those commands are not replicated, and should be applied on each node
   separately.

   Unsupported in this demo:
   commands: LOAD DATA.
   features: LOCKing sessions.
   Use at your own risk.

