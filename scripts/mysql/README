Codership Oy
http://www.codership.com
<info@codership.com>

DISCLAIMER

THIS DEMO SOFTWARE PROVIDED "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER
EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.
IN NO EVENT SHALL CODERSHIP OY BE HELD LIABLE TO ANY PARTY FOR ANY DAMAGES
RESULTING DIRECTLY OR INDIRECTLY FROM THE USE OF THIS SOFTWARE.

Trademark Information.

MySQL is a trademark or registered trademark of Sun Microsystems, Inc. or
its subsidiaries in the US and other countries. Other marks are the property
of their respective owners.

Licensing Information.

Please see ./mysql/LICENSE.mysql and ./galera/LICENSE.galera

ABOUT THIS DOCUMENT

This document only covers issues specific to MySQL/Galera demo. It does not cover the use or administration of MySQL server per se. The reader is assumed to know how to install, configure, administer and use MySQL server.


CONTENTS:
=========
0. WHAT IS MYSQL/GALERA CLUSTER
1. DEMO SETUP
   1.1 DEMO INSTALLATION
   1.2 STARTING GROUP COMMUNICATION DAEMON
   1.3 STARTING MYSQL SERVERS
2. USING THE CLUSTER
   2.1 LOADING DATA TO A CLUSTER
   2.2 CONNECTING APPLICATION TO A CLUSTER
   2.3 LOAD BALANCER
   2.4 ADDING NEW NODE TO A CLUSTER
3. CONFIGURATION
   3.1 MANDATORY OPTIONS
   3.2 WSREP OPTIONS
4. Using MySQL/Galera demo in Amazon EC2
5. LIMITATIONS


0. WHAT IS MYSQL/GALERA CLUSTER

MySQL/Galera demo cluster is a synchronous multi-master MySQL server cluster.
It consists of a single group communication daemon gcomm and several MySQL 
server instances which connect to it in a star-like fashion:

     ,--------.   ,---------.   ,--------.
     | mysqld |---|  gcomm  |---| mysqld |
     `--------'   `---------'   `--------'
                   /       \
            ,--------.   ,--------.
            | mysqld |   | mysqld |
            `--------'   `--------'

Server states are synchronized by replicating transaction changes at commit time. The cluster is virtually synchronous: this means that each node commits transactions in exactly the same order, although not necessarily at the same physical moment. (The latter is not that important as it may seem, since in most cases dbms gives no guarantee on when the transaction is actually executed.) Built-in flow control keeps nodes within fraction of a second from each other, this is more than enough for most practical purposes.

Main features of MySQL/Galera cluster:

* Truly highly available: no transaction is ever lost in case of a node crash. 
  All nodes always have consistent state.

* True multi-master: all cluster nodes can handle WRITE load concurrently.

* Highly transparent: the cluster is intended as a drop-in replacement for a 
  single MySQL server. (See LIMITATIONS below)

* Scalable even with WRITE-intensive applications.

This demo package contains all software you'll need to setup MySQL/Galera cluster. It is essentially a self-contained MySQL server installation with its own configuration file, data directory and preconfigured empty database.
You don't need administrative privileges or to uninstall/disable previously
installed MySQL software to use this demo.


1. DEMO SETUP
               
To setup MySQL/Galera demo cluster you will need several networked computers - one for each mysqld instance you plan to use. For best performance those computers
should be of approximately same configuration: Galera replication is synchronous
and one slow machine will slow down the whole cluster.

You must also select one machine that would host group communication daemon
(hereafter arbitrator). For best performance it should be a separate computer,
but it also can be one of the MySQL nodes. The only requirement is that
arbitrator is accessible to all other cluster nodes.

For your convenience this demo 

It takes 3 steps to set up the cluster:
1) Copy demo distribution to all prospective nodes of the cluster and unpack it
   to location of your choice.
2) Start group communication daemon on arbitrator node by running gcomm script
   from the distribution.
3) Start MySQL servers.

(NOTE: You can easily set up the cluster on a single computer. However this makes little sense, as you won't see the the benefits of high availability and scalability. Hence it is not covered by this document.)


1.1 DEMO INSTALLATION

Just copy and unpack the demo on the prospective cluster nodes to wherever you have privileges. The demo was designed to be able to run on most systems without reconfiguration. It is a self-contained MySQL installation and comes with its own data directory and a preconfigured empty database with users 'root' (password 'rootpass') and 'test' (password 'testpass', privileges on schema 'test'). As a result default installation will require at least 1Gb of free space for InnoDB files (will be created on first start).

This requirement, as well as other MySQL and Galera options can be changed by editing configuration file which can be found at <DEMO_ROOT>/mysql/etc/my.cnf.

Please see CONFIGURATION chapter for the details on editable parameters.


1.2 STARTING GROUP COMMUNICATION DAEMON

Select one of the nodes to act as an arbitrator. On that node issue the following command:

   <DEMO_ROOT>/gcomm start

This will start a Galera group communication daemon listening on all interfaces at port 4567. GCS_ADDRESS environment variable can be used to bind the daemon to particular interface or port, e.g.:

   GCS_ADDRESS=tcp:192.168.0.1:3333 <DEMO_ROOT>/gcomm start


1.3 STARTING MYSQL SERVERS

<DEMO_ROOT>/mysql-galera is a special MySQL startup script that sets proper options (including data directory path) for mysqld. If you're running it as a superuser, you have to make sure there is 'mysql' user in the system and it has sufficient privileges on the demo directory (see MySQL Manual about running mysqld as root). 

You will need to tell mysqld the address of the group communication daemon. For the purpose of this demo the address follows this layout: gcomm://tcp:<IP address>:<port>. Thus, if you have started gcomm on the node with IP address 192.168.1.1, then GCS address would be gcomm://tcp:192.168.1.1:4567

1) Either supply the address as the option on the command line:

   <DEMO_ROOT>/mysql-galera -g gcomm://tcp:192.168.1.1:4567 start

2) Or you can hardcode it in <DEMO_ROOT>/my.cnf by setting
   wsrep_gcs_address="gcomm://tcp:192.168.1.1:4567" and then just starting
   launching mysql-galera script:

   <DEMO_ROOT>/mysql-galera start

It might take few minutes to start mysqld for the first time as it will have to create required InnoDB files.

For full description of mysql-galera options and commands see:

   <DEMO_ROOT>/mysql-galera --help


2. USING THE CLUSTER

After you have successfully started all cluster nodes, the cluster is ready to
use.

From the client point of view each cluster node works like a usual MySQL server - client-side application does not have to be changed in any way. Each node can be accessed independently and asynchronously. Just direct SQL load to any one or more of the cluster nodes. For most practical purposes you can treat MySQL/Galera cluster as a single MySQL server listening on multiple interfaces with the exception that you might see transaction deadlocks where you previously hadn't.

2.1 LOADING DATA TO CLUSTER

Initially demo database is empty. You can populate it by loading the dump of your data to any one of the nodes. It will be automatically replicated to others. Please note that this demo supports only InnoDB storage engine.

2.2 CONNECTING APPLICATION TO CLUSTER

As was mentioned above, for the client application each node looks like a normal MySQL server and can be used independently. This creates considerable flexibility in the way the cluster can be utilized. The approaches can be categorized in three groups:

1) Seeking High Availability only. In this case client application connects to
   only one node, the rest serving as hot backups:

   ,-------------.
   | application |
   `-------------'
        | | |        DB backups
      ,-------. ,-------. ,-------.
      | node1 | | node2 | | node3 |
      `-------' `-------' `-------'
       <===== cluster nodes =====>

   In the case of primary node failure application can instantly switch to
   another node.

2) Seeking High Availability and improved performance through uniform load
   distribution. If there are several client connections to the database, they
   can be uniformly distributed between cluster nodes resulting in better
   performance. The exact degree of performance improvement depends on
   application's SQL profile. Note, that transaction rollback rate may also 
   increase.

             ,-------------.
             | application |
             `-------------'
             /      |      \
      ,-------. ,-------. ,-------.
      | node1 | | node2 | | node3 |
      `-------' `-------' `-------'
       <===== cluster nodes =====>

   In the case of a node failure application can keep on using the remaining 
   healthy nodes.

   In this setup application can also be clustered with a dedicated application
   instance per database node, thus achieving HA not only for the database,
   but for the whole application stack:

             ,-------------.
             |   clients   |
             `-------------'
             /      |      \
      ,------.   ,------.   ,------.
      | app1 |   | app2 |   | app3 |
      `------'   `------'   `------'
         |          |          |
     ,-------.  ,-------.  ,-------.
     | node1 |  | node2 |  | node3 |
     `-------'  `-------'  `-------'
      <====== cluster nodes ======>

3) Seeking High Availability and improved performance through smart load
   distribution / data partitioning. If your application can make use of data
   partitioning, it can be configured to use each cluster node for one or more
   partitions. Because in Galera cluster each node has a complete copy of data,
   a node loss does not mean partition data loss - partitions can be instantly
   rearranged among the remaining nodes. Normally partitioning would improve
   performance better than just uniform load distribution.


2.3 LOAD BALANCER

If your application cannot utilize several database servers (most don't) you will need to use SQL proxy or a TCP load balancer to distribute load between the MySQL/Galera nodes. This is needed not only to increase performance, but also for a quick switch in case of a node failure. If performance of your application is DBMS-bound, you can run the balancer on the same machine as application/client. Be aware, however, that SQL load balancing might be a CPU hungry operation: usually SQL traffic consists of many small packets. For best results we recommend to carefully examine CPU consumption of the balancer and if needed dedicate a separate machine for it.

Unlike traditional MySQL master-slave cluster MySQL/Galera cluster does not require any SQL traffic filtering or redirection of write requests - it is a truly multi-master cluster. As such it does not require SQL-parsing proxies to operate correctly and plain TCP connection balancer would suffice. (However under certain conditions intelligent load balancing may improve performance.)

TCP connection balancers that were successfully used with MySQL/Galera:
- GLB (http://www.codership.com/products/downloads) 
- Pen (http://siag.nu/pen/)


2.4 ADDING NEW NODE TO A CLUSTER

This demo does not support automatic node provisioning (yet), so adding new node to a working cluster is a somewhat disruptive operation. You will need to take the following steps to do it:

1) Install demo on a new node as usual - but don't start mysqld.
2) Connect the node to a cluster network. Make sure it can connect to arbitrator.
3) Stop SQL load on cluster.
4) Connect with mysql client to one of the nodes and issue
   FLUSH TABLES WITH READ LOCK;
   command. Don't close this session.
5) Copy data directory from that server to the new node.
6) Start mysqld on the new node.
7) Close the locking session from step 4.
8) Resume SQL load.


3. CONFIGURATION

Each MySQL/Galera node is configured just like the usual MySQL server, we just added few configuration variables to my.cnf. In addition some options can be passed to mysql-galera startup script (see mysql-galera --help).

3.1 MANDATORY CONFIGURATION OPTIONS

log_bin=mysql-bin
binlog_format=ROW
Those two options are required to use ROW-level replication as opposed to statement-level. For performance and consistency considerations don't change that. As a side effect, binlog is temporarily disabled. In future those options won't have special meaning.

innodb_lock_wait_timeout=9999999
This option is required for correct conflict resolution.

Mandatory options are hardcoded both in demo my.cnf file and in mysql-galera script.

3.2 WSREP OPTIONS

Here WSREP stands for Write-Set REPlication - a synchronous replication API that Codership is developing for transactional databases. Galera is a library that implements WSREP services. Default values are shown.

wsrep_provider=none
   A full path to the library that implements WSREP interface. If none is
   specified, the server behaves almost like a normal mysqld, with slight
   overhead. mysql-galera script automatically substitutes it to point to
   Galera implementation shipped with the demo. It can be overridden with
   WSREP environment variable.

wsrep_gcs_address="dummy://"
   Group Communication System address. Depends on the WSREP provider. Galera
   recognises "dummy://" and "gcomm://tcp:<IP address>:<IP port>"

wsrep_gcs_group="my_cluster"
   Logical group name, must be the same for all nodes of the cluster.

wsrep_slave_threads=1
   Number of threads dedicated to processing of writesets from other nodes.
   Values greater than 1 not tested.

wsrep_dbug_option
   Options for the built-in DBUG library (independent from what MySQL uses).
   Empty by default.

wsrep_local_cache_size=16777216
   Amount of RAM reserved for writeset cache in bytes. Excess writesets are
   stored on the hard drive in MySQL data directory

wsrep_ws_persistency=0

wsrep_debug=0



4. Using MySQL/Galera demo in Amazon EC2

MySQL/Galera demo works anywhere TCP/IP works. Therefore using MySQL/Galera demo in Amazon EC2 environment is no different than in LAN. Just launch several instances of your favourite AMI, copy and unpack the distribution, start gcomm on one of them and then start servers telling them where to find the arbitrator. Don't forget to use external arbitrator address if your nodes are running in different accessibility zones (obviously running in different accessibility zones degrades performance somewhat).

NOTE: this demo may be binary incompatible with some older Linux distributions.
      Please use CentOS 5.0 or newer.

5. LIMITATIONS

1) Currently replication works only with InnoDB storage engine. Any writes to 
   tables of other types, including system (mysql.*) tables are not replicated. 
   E.g. if you want to add another user, you have to do it manually on every
   node where you want that user to connect to.

2) Rows in tables without primary keys may appear in different order on different
   nodes. As a result SELECT...LIMIT... may return slightly different sets.

3) Currently MySQL/Galera demo lacks automatic state transfer. Every time 
   you add a new node to the cluster and/or restart a node you need to manually 
   synchronize database contents on the nodes. See README for details.

4) Unsupported in this demo:
   commands: LOAD DATA.
   features: LOCKing sessions.
   Use at your own risk.

